<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Muhammad Angga Muttaqien">
    <title>Muhammad Angga Muttaqien</title>
    <link rel="stylesheet" href="assets/css/style.css">
  </head>

  <body>
    <div class="container">
      <!-- Header -->
      <section class="header">
        <div class="photo">
          <img src="assets/img/profile.jpg" alt="Profile photo" />
        </div>
        <div class="intro">
          <h1>Muhammad Angga Muttaqien</h1>
          <p>
            I am a <strong>Research Assistant</strong> at the 
            <strong>Embodied AI Research Team</strong>, 
            <strong>AIST (産業技術総合研究所)</strong>. 
            My research focuses on <strong>foundation models</strong>, 
            <strong>world models</strong>, and 
            <strong>deep reinforcement learning</strong>. 
            I aim to integrate multimodal reasoning, perception, and control 
            toward developing general-purpose robotic systems capable of 
            adaptive and autonomous behavior.
          </p>
          <p>
            I hold a Master's degree in <strong>AI and Robotics</strong> from the <strong>University of Tsukuba</strong>, 
            completed under the <strong>Human-Centered AI Program funded by MEXT</strong>. During my studies,
            I developed curriculum-guided deep reinforcement learning frameworks for home robots capable of natural language interaction.
          </p>
          <p>
            <a href="mailto:muhamuttaqien@gmail.com">muhamuttaqien@gmail.com</a> |
            <a href="https://github.com/muhamuttaqien">GitHub</a> |
            <a href="https://scholar.google.com">Scholar</a> |
            <a href="https://medium.com">Blog</a>
          </p>
        </div>
      </section>

      <!-- Life Updates -->
      <section class="updates">
        <div class="updates-header">Recent Updates</div>
        <ul>
          <li><strong>[October 2025]</strong> Submitted a paper to IEEE RO-MAN 2025 on language grounding in robotic manipulation.</li>
          <li><strong>[July 2025]</strong> Presented research on curriculum-based reinforcement learning at IEEE/SICE SII 2025.</li>
          <li><strong>[April 2025]</strong> Joined the <strong>Embodied AI Research Team</strong> at AIST Japan.</li>
        </ul>
      </section>

      <!-- Research -->
      <section class="section">
        <h2>Research</h2>
        <p>
          My research investigates how <strong>world models</strong> can enhance robot learning and safety in real-world environments.
          I focus on bridging <strong>language</strong>, <strong>vision</strong>, and <strong>action</strong> through reinforcement learning
          and multimodal representation learning, aiming to create transparent and generalizable AI systems.
        </p>
      </section>

      <!-- Publications -->
      <section class="section">
        <h2>Publications</h2>
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project1.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Curriculum-based Smart Home Robot Navigation with Natural Language Interaction</span></a><br>
            <strong>Muhammad Angga Muttaqien</strong><br>
            <em>IEEE/SICE SII 2025</em><br>
            <p>
              <strong>Abstract:</strong> A deep reinforcement learning framework enabling robots to execute
              natural language navigation tasks through curriculum-based multimodal training.
            </p>
          </div>
        </div>

        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project2.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Visual Prompting for Robotic Manipulation using Foundation Models</span></a><br>
            <strong>Muhammad Angga Muttaqien</strong><br>
            <em>IEEE CASE 2025</em><br>
            <p>
              <strong>Abstract:</strong> A visual-language prompting method for robust robotic manipulation,
              combining GR00T and ACT models under variable environmental conditions.
            </p>
          </div>
        </div>
      </section>

      <!-- Projects -->
      <section class="section">
        <h2>Projects</h2>
        <ul>
          <li><strong>DreamerCL</strong> — Curriculum-guided world model learning for long-horizon embodied agents.</li>
          <li><strong>ALOHA + GR00T</strong> — Multimodal foundation model integration for robotic manipulation tasks.</li>
        </ul>
      </section>

      <footer>
        <p>© 2025 Muhammad Angga Muttaqien — Hosted on GitHub Pages</p>
      </footer>
    </div>
  </body>
</html>
