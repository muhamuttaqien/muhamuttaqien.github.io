<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>Muhammad Angga Muttaqien | AI & Robotics Researcher</title>
  <link rel="stylesheet" href="assets/css/style.css" />
</head>
<body>
  <div class="container">

    <!-- Introduction section -->
    <section class="intro">
      <div class="profile-photo">
        <img src="assets/img/profile.jpg" alt="Profile photo" />
      </div>

      <div class="bio">
        <h1>Muhammad Angga Muttaqien</h1>
        <p>
          I am a researcher at the <strong>Embodied AI Research Team, AIST Japan</strong>,
          focusing on <strong>foundation models</strong>, <strong>embodied intelligence</strong>,
          and <strong>curriculum-based learning</strong>. My work integrates multimodal reasoning,
          perception, and control toward building general-purpose robotic systems.
        </p>
        <p>
          Previously, I earned my Master's degree in <strong>AI and Robotics</strong> from the
          University of Tsukuba, where my research explored <strong>curriculum-guided deep
          reinforcement learning</strong> for natural language-based robot navigation.
        </p>
        <p>
          üì´ <a href="mailto:muhamuttaqien@gmail.com">muhamuttaqien@gmail.com</a><br>
          üîó <a href="https://github.com/muhamuttaqien">GitHub</a> /
          <a href="https://scholar.google.com">Scholar</a> /
          <a href="https://medium.com">Blog</a>
        </p>
      </div>
    </section>

    <hr>

    <!-- Research Section -->
    <section>
      <h2>Research</h2>
      <p>
        My research focuses on designing <strong>world models</strong> for embodied AI agents,
        combining <strong>reinforcement learning</strong>, <strong>language grounding</strong>,
        and <strong>robotic manipulation</strong>. I aim to enable robots to learn from
        multimodal inputs‚Äîvision, language, and action‚Äîwithin a unified, interpretable framework.
      </p>
    </section>

    <!-- Publications -->
    <section>
      <h2>Publications</h2>
      <ul>
        <li>
          <strong>Curriculum-based Smart Home Robot Navigation with Natural Language Interaction using End-to-End Deep Reinforcement Learning</strong><br>
          <em>IEEE/SICE SII 2025</em>
        </li>
        <li>
          <strong>Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</strong><br>
          <em>IEEE CASE 2025</em>
        </li>
      </ul>
    </section>

    <!-- Projects -->
    <section>
      <h2>Projects</h2>
      <ul>
        <li><strong>DreamerCL</strong> ‚Äî Curriculum-guided World Model Learning for Embodied Agents</li>
        <li><strong>GR00T + ALOHA</strong> ‚Äî Visual-language-action foundation model adaptation for robotic manipulation</li>
      </ul>
    </section>

    <footer>
      <p>¬© 2025 Muhammad Angga Muttaqien ‚Äî Built with ‚ù§Ô∏è and GitHub Pages</p>
    </footer>
  </div>
</body>
</html>
