<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Muhammad Angga Muttaqien">
    <title>Muhammad Angga Muttaqien</title>
    <link rel="stylesheet" href="assets/css/style.css">
  </head>

  <body>
    <div class="container">
      <!-- Header -->
      <section class="header">
        <div class="photo">
          <img src="assets/img/profile.jpg" alt="Profile photo" />
        </div>
        <div class="intro">
          <h1>Muhammad Angga Muttaqien</h1>
          <p>
            I am a <strong>Research Assistant</strong> at the 
            <a href="https://unit.aist.go.jp/icps/icps-am/research/" target="_blank"><strong>Embodied AI Research Team</strong></a>, 
            <strong>National Institute of AIST</strong> (Áî£Ê•≠ÊäÄË°ìÁ∑èÂêàÁ†îÁ©∂ÊâÄ), working under the supervision of 
            <a href="https://researchmap.jp/r-hanai/" target="_blank"><strong>Dr. Ryo Hanai</strong></a> and 
            <a href="https://tomohiromotoda.github.io/" target="_blank"><strong>Dr. Tomohiro Motoda</strong></a>. 
            My research focuses on <strong>foundation models</strong>, 
            <strong>world models</strong>, and 
            <strong>deep reinforcement learning</strong> for building general-purpose robots.
          </p>
          
          <p>
            I hold a <strong>Master‚Äôs degree in AI and Robotics</strong> from the 
            <strong>University of Tsukuba</strong>, where I studied under 
            <a href="https://www.cs.tsukuba.ac.jp/~ohya/index_jp.html" target="_blank"><strong>Prof. Akihisa Ohya</strong></a> as part of the 
            <a href="https://www.cs.tsukuba.ac.jp/cs-p-curriculum/hcaip-jp/" target="_blank"><strong>Human-Centered AI Program funded by MEXT</strong></a>. 
            My graduate research explored curriculum-guided deep reinforcement learning for home robots with natural language interaction.
          </p>

          <p>
            <a href="mailto:muha.muttaqien@aist.go.jp" target="_blank">muha.muttaqien@aist.go.jp</a> |
            <a href="https://drive.google.com/file/d/1BTquTMM3XaU4GLTtdgBU0Gc8LnXMOP1B/view?usp=sharing" target="_blank">CV</a> |
            <a href="https://scholar.google.com/citations?user=NA9hcLQAAAAJ" target="_blank">Scholar</a> |
            <a href="https://github.com/muhamuttaqien" target="_blank">GitHub</a> 
          </p>
        </div>
      </section>

      <!-- Life Updates -->
      <section class="updates">
        <div class="updates-header">Recent Updates</div>
        <ul>
          <ul>
            <li><strong>[August 2025]</strong> Presented our research on robotic manipulation at 
              <a href="https://2025.ieeecase.org/" target="_blank"><strong>IEEE CASE 2025</strong></a>, Los Angeles, US.
            </li>
            <li><strong>[April 2025]</strong> Officially joined the 
              <a href="https://unit.aist.go.jp/icps/icps-am/people/" target="_blank"><strong>Embodied AI Research Team</strong></a> at <strong>AIST Tokyo</strong>.</li>
            <li><strong>[March 2025]</strong> Graduated from the <strong>University of Tsukuba</strong> with a Master‚Äôs degree in engineering.</li>
            <li><strong>[January 2025]</strong> Presented our research on multimodal perception at 
              <a href="https://sice-si.org/SII2025/" target="_blank"><strong>IEEE/SICE SII 2025</strong></a>, Munich, Germany.
            </li>
          </ul>
        </ul>
      </section>

      <!-- Research -->
      <section class="section">
        <h2>Research</h2>
        <p>
          My research focuses on <strong>embodied artificial intelligence</strong>, integrating <strong>foundation models</strong>, <strong>world models</strong>, and <strong>deep reinforcement learning</strong> to build general-purpose robots. 
          I envision future AI robots that can perform with <strong>unseen objects</strong>, <strong>layouts</strong>, and <strong>tasks</strong>, while interacting with humans seamlessly (and safely) in the real world.
        </p>
      </section>

      <!-- Publications -->
      <section class="section">
        <h2>Publications</h2>
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/image-preview.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="https://ieeexplore.ieee.org/abstract/document/11164002" target="_blank"><span class="paper-title">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</span></a><br>
            <span class="authors">Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</span><br>
            <em>2025 IEEE International Conference on Automation Science and Engineering (CASE), Los Angeles, US</em><br>
            <p>
              <strong>TLDR:</strong> A perception-action pipeline using annotation-guided visual prompting and Action Chunking with Transformers (ACT) enables adaptive, data-driven pick-and-place operations in cluttered retail environments with improved grasp accuracy and success rates.
            </p>
          </div>
        </div>
        
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/image-preview.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="https://ieeexplore.ieee.org/abstract/document/10871055" target="_blank"><span class="paper-title">Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation</span></a><br>
            <span class="authors">Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</span><br>
            <em>2025 IEEE/SICE International Symposium on System Integrations (SII), Munich, Germany</em><br>
            <p>
              <strong>TLDR:</strong> A structured pipeline integrating CLIP, SAM, and gradient-based attention enhances object masking precision for convenience store products, enabling more accurate and adaptive robotic manipulation through multimodal fine-tuning and effective image-text alignment.
            </p>
          </div>
        </div>

        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/image-preview.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="https://ieeexplore.ieee.org/abstract/document/10673231" target="_blank"><span class="paper-title">Mobile Robots through Task-Based Human Instructions using Incremental Curriculum Learning</span></a><br>
            <span class="authors">Muhammad A. Muttaqien, Ayanori Yorozu, Akihisa Ohya</span><br>
            <em>2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS), Hangzhou, China</em><br>
            <p>
              <strong>TLDR:</strong> An incremental curriculum learning (ICL) framework combined with deep reinforcement learning (DRL) enables robots to progressively master task-based navigation from human instructions, improving training efficiency, generalization, and performance in dynamic indoor environments.
            </p>
          </div>
        </div>

        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/image-preview.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="https://ieeexplore.ieee.org/abstract/document/11164002" target="_blank"><span class="paper-title">Precision and Adaptability of YOLOv5 and YOLOv8 in Dynamic Robotic Environments</span></a><br>
            <span class="authors">Victor A. Kich, Muhammad A. Muttaqien, Junya Toyama, Ryutaro Miyoshi, Yosuke Ida, Akihisa Ohya, Hisashi Date</span><br>
            <em>2024 IEEE International Conference on Cybernetics and Intelligent Systems (CIS), Hangzhou, China</em><br>
            <p>
              <strong>TLDR:</strong> A comparative analysis of YOLOv5 and YOLOv8 reveals that YOLOv5 can match or surpass YOLOv8 in precision for robotic object detection, highlighting the importance of architectural simplicity, dataset factors, and contextual evaluation in selecting real-time detection frameworks.
            </p>
          </div>
        </div>
      </section>

      <!-- Projects -->
      <section class="section">
        <h2>Projects</h2>
      
        <!-- Project 1 -->
        <div class="project">
          <div class="project-image">
            <img src="assets/img/image-preview.jpg" alt="Project preview">
          </div>
          <div class="project-text">
            <a href="javascript:void(0);"><span class="paper-title">ALOHA + GR00T System Integration</span></a><br>
            <span class="project-subtitle">2025 ‚Äî National Institute of AIST</span><br>
            <p>
              <strong>TLDR:</strong> This project integrates <strong>ALOHA</strong> (a bimanual robotic manipulator) with a 
              <strong>fine-tuned GR00T foundation model</strong> using our prepared demonstration data to perform pick-and-place operations
              on diverse products in Japanese convenience stores.
            </p>
          </div>
        </div>
      
        <!-- Project 2 -->
        <div class="project">
          <div class="project-image">
            <img src="assets/img/image-preview.jpg" alt="Project preview">
          </div>
          <div class="project-text">
            <a href="javascript:void(0);"><span class="paper-title">Tsukuba Challenge 2023</span></a><br>
            <span class="project-subtitle">2023 ‚Äî University of Tsukuba</span><br>
            <p>
              <strong>TLDR:</strong> Collaborated with colleagues from the 
              <a href="https://www.roboken.iit.tsukuba.ac.jp/" target="_blank"><strong>Intelligent Robot Laboratory</strong></a> at the 
              <strong>University of Tsukuba</strong> to develop a mobile robot capable of seamless navigation 
              along predefined outdoor routes. The team successfully completed the challenge and was recognized 
              among the top-performing entries in the competition. üéâ
            </p>
          </div>
        </div>
      
        <!-- Project 3 -->
        <div class="project">
          <div class="project-image">
            <img src="assets/img/image-preview.jpg" alt="Projects preview">
          </div>
          <div class="project-text">
            <a href="javascript:void(0);"><span class="paper-title">Industrial AI Projects</span></a><br>
            <span class="project-subtitle">2018-2021 ‚Äî GRID Co., Ltd. (Ê†™Âºè‰ºöÁ§æ„Ç∞„É™„ÉÉ„Éâ), Tokyo</span><br>
            <p>
              <strong>TLDR:</strong> As an <strong>AI Research Engineer</strong>, 
              contributed to multiple industrial AI applications, including 
              <strong>Unit Commitment Automation</strong> for thermal and hydro power systems, 
              <strong>mechanical drawing analysis</strong>, <strong>material property segmentation</strong>, and 
              <strong>road damage detection</strong> for 
              <a href="https://www.nikkei.com/article/DGXMZO61941140X20C20A7000000/" target="_blank"><strong>Nichireki Corporation</strong></a>.
            </p>
          </div>
        </div>
      </section>

      <section class="section">
        <h2>About Me</h2>
        <p>
          I was born and raised in Indonesia and am currently based in Tokyo, Japan. Beyond research, I enjoy exploring the intersection between
          <strong>technology, philosophy, and human understanding</strong> ‚Äîreflecting on how AI as the most disruptive technology of the 21st century shapes our perception of knowledge and human thinking.
          In my spare time, I like reading thought-provoking books (such as <em>Sapiens</em> and <em>Homo Deus</em>), writing analytical essays, and playing strategy-based games.
        </p>
      </section>

      <footer>
        <p>¬© 2025 Muhammad Angga Muttaqien (muhamuttaqien) ‚Äî Hosted on GitHub Pages</p>
      </footer>
    </div>
  </body>
</html>
