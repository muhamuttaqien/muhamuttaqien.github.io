<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Muhammad Angga Muttaqien">
    <title>Muhammad Angga Muttaqien</title>
    <link rel="stylesheet" href="assets/css/style.css">
  </head>

  <body>
    <div class="container">
      <!-- Header -->
      <section class="header">
        <div class="photo">
          <img src="assets/img/profile.jpg" alt="Profile photo" />
        </div>
        <div class="intro">
          <h1>Muhammad Angga Muttaqien</h1>
          <p>
            I am a <strong>Research Assistant</strong> at the 
            <a href="https://unit.aist.go.jp/icps/icps-am/research/" target="_blank"><strong>Embodied AI Research Team</strong></a>, 
            <strong>National Institute of AIST</strong> (産業技術総合研究所), working under the supervision of 
            <a href="https://researchmap.jp/r-hanai/" target="_blank"><strong>Dr. Ryo Hanai</strong></a> and 
            <a href="https://tomohiromotoda.github.io/" target="_blank"><strong>Dr. Tomohiro Motoda</strong></a>. 
            My research focuses on <strong>foundation models</strong>, 
            <strong>world models</strong>, and 
            <strong>deep reinforcement learning</strong> for building general-purpose robots.
          </p>
          
          <p>
            I hold a <strong>Master’s degree in AI and Robotics</strong> from the 
            <strong>University of Tsukuba</strong>, where I studied under 
            <a href="https://www.cs.tsukuba.ac.jp/~ohya/index_jp.html" target="_blank"><strong>Prof. Akihisa Ohya</strong></a> as part of the 
            <a href="https://www.cs.tsukuba.ac.jp/cs-p-curriculum/hcaip-jp/" target="_blank"><strong>Human-Centered AI Program funded by MEXT</strong></a>. 
            My graduate research explored curriculum-guided deep reinforcement learning for home robots with natural language interaction.
          </p>

          <p>
            <a href="mailto:muha.muttaqien@aist.go.jp" target="_blank">muha.muttaqien@aist.go.jp</a> |
            <a href="https://drive.google.com/file/d/1BTquTMM3XaU4GLTtdgBU0Gc8LnXMOP1B/view?usp=sharing" target="_blank">CV</a> |
            <a href="https://scholar.google.com/citations?user=NA9hcLQAAAAJ" target="_blank">Scholar</a> |
            <a href="https://github.com/muhamuttaqien" target="_blank">GitHub</a> 
          </p>
        </div>
      </section>

      <!-- Life Updates -->
      <section class="updates">
        <div class="updates-header">Recent Updates</div>
        <ul>
          <ul>
            <li><strong>[August 2025]</strong> Presented our research on robotic manipulation at 
              <a href="https://2025.ieeecase.org/" target="_blank"><strong>IEEE CASE 2025</strong></a>, Los Angeles, US.
            </li>
            <li><strong>[April 2025]</strong> Officially joined the <strong>Embodied AI Research Team</strong> at <strong>AIST Tokyo</strong>.</li>
            <li><strong>[March 2025]</strong> Graduated from the <strong>University of Tsukuba</strong> with a Master’s degree in engineering.</li>
            <li><strong>[January 2025]</strong> Presented our research on multimodal perception at 
              <a href="https://sice-si.org/SII2025/" target="_blank"><strong>IEEE/SICE SII 2025</strong></a>, Munich, Germany.
            </li>
          </ul>
        </ul>
      </section>

      <!-- Research -->
      <section class="section">
        <h2>Research</h2>
        <p>
          My research focuses on <strong>embodied artificial intelligence</strong>, integrating <strong>foundation models</strong>, <strong>world models</strong>, and <strong>deep reinforcement learning</strong> to build general-purpose robots. 
          I envision future robots that can perform with <strong>unseen objects</strong>, <strong>layouts</strong>, and <strong>tasks</strong>, while interacting with humans seamlessly (and safely) in the real world.
        </p>
      </section>

      <!-- Publications -->
      <section class="section">
        <h2>Publications</h2>
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project1.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Visual Prompting for Robotic Manipulation with Annotation-Guided Pick-and-Place Using ACT</span></a><br>
            <span class="authors">Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</span><br>
            <em>IEEE CASE 2025</em><br>
            <p>
              <strong>Abstract:</strong> A visual-language prompting method for robust robotic manipulation,
              combining GR00T and ACT models under variable environmental conditions.
            </p>
          </div>
        </div>
        
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project2.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Attention-Guided Integration of CLIP and SAM for Precise Object Masking in Robotic Manipulation</span></a><br>
            <span class="authors">Muhammad A. Muttaqien, Tomohiro Motoda, Ryo Hanai, Yukiyasu Domae</span><br>
            <em>IEEE/SICE SII 2025</em><br>
            <p>
              <strong>Abstract:</strong> A deep reinforcement learning framework enabling robots to execute
              natural language navigation tasks through curriculum-based multimodal training.
            </p>
          </div>
        </div>
      </section>

      <!-- Projects -->
      <section class="section">
        <h2>Projects</h2>
        <ul>
          <li><strong>DreamerCL</strong> — Curriculum-guided world model learning for long-horizon embodied agents.</li>
          <li><strong>ALOHA + GR00T</strong> — Multimodal foundation model integration for robotic manipulation tasks.</li>
        </ul>
      </section>

      <footer>
        <p>© 2025 Muhammad Angga Muttaqien — Hosted on GitHub Pages</p>
      </footer>
    </div>
  </body>
</html>
