<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <meta name="author" content="Muhammad Angga Muttaqien">
    <title>Muhammad Angga Muttaqien</title>
    <link rel="stylesheet" href="assets/css/style.css">
  </head>

  <body>
    <div class="container">
      <!-- Header -->
      <section class="header">
        <div class="photo">
          <img src="assets/img/profile.jpg" alt="Profile photo" />
        </div>
        <div class="intro">
          <h1>Muhammad Angga Muttaqien</h1>
          <p>
            I am a <strong>Research Assistant</strong> at the 
            <a href="https://unit.aist.go.jp/icps/icps-am/research/" target="_blank"><strong>Embodied AI Research Team</strong></a>, 
            <strong>AIST</strong>, working under the supervision of 
            <a href="https://researchmap.jp/r-hanai/" target="_blank"><strong>Dr. Ryo Hanai</strong></a> and 
            <a href="https://tomohiromotoda.github.io/" target="_blank"><strong>Dr. Tomohiro Motoda</strong></a>. 
            My research focuses on <strong>foundation models</strong>, 
            <strong>world models</strong>, and 
            <strong>deep reinforcement learning</strong>. 
            I integrate multimodal reasoning, perception, and control to build general-purpose robots.
          </p>
          
          <p>
            I hold a <strong>Master’s degree in AI and Robotics</strong> from the 
            <strong>University of Tsukuba</strong>, where I studied under 
            <a href="https://www.cs.tsukuba.ac.jp/~ohya/index_jp.html" target="_blank"><strong>Prof. Akihisa Ohya</strong></a> as part of the 
            <a href="https://www.cs.tsukuba.ac.jp/cs-p-curriculum/hcaip-jp/" target="_blank"><strong>Human-Centered AI Program funded by MEXT</strong></a>. 
            My graduate research explored curriculum-guided deep reinforcement learning for home robots with natural language interaction.
          </p>

          <p>
            <a href="mailto:muha.muttaqien@aist.go.jp">muha.muttaqien@aist.go.jp</a> |
            <a href="https://github.com/muhamuttaqien">GitHub</a> |
            <a href="https://scholar.google.com/citations?user=NA9hcLQAAAAJ">Scholar</a> |
            <a href="https://drive.google.com/file/d/1BTquTMM3XaU4GLTtdgBU0Gc8LnXMOP1B/view?usp=sharing">CV</a>
          </p>
        </div>
      </section>

      <!-- Life Updates -->
      <section class="updates">
        <div class="updates-header">Recent Updates</div>
        <ul>
          <li><strong>[October 2025]</strong> Submitted a paper to IEEE RO-MAN 2025 on language grounding in robotic manipulation.</li>
          <li><strong>[July 2025]</strong> Presented research on curriculum-based reinforcement learning at IEEE/SICE SII 2025.</li>
          <li><strong>[April 2025]</strong> Joined the <strong>Embodied AI Research Team</strong> at AIST Japan.</li>
        </ul>
      </section>

      <!-- Research -->
      <section class="section">
        <h2>Research</h2>
        <p>
          My research investigates how <strong>world models</strong> can enhance robot learning and safety in real-world environments.
          I focus on bridging <strong>language</strong>, <strong>vision</strong>, and <strong>action</strong> through reinforcement learning
          and multimodal representation learning, aiming to create transparent and generalizable AI systems.
        </p>
      </section>

      <!-- Publications -->
      <section class="section">
        <h2>Publications</h2>
        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project1.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Curriculum-based Smart Home Robot Navigation with Natural Language Interaction</span></a><br>
            <strong>Muhammad Angga Muttaqien</strong><br>
            <em>IEEE/SICE SII 2025</em><br>
            <p>
              <strong>Abstract:</strong> A deep reinforcement learning framework enabling robots to execute
              natural language navigation tasks through curriculum-based multimodal training.
            </p>
          </div>
        </div>

        <div class="publication">
          <div class="pub-image">
            <img src="assets/img/project2.jpg" alt="Research preview">
          </div>
          <div class="pub-text">
            <a href="#"><span class="paper-title">Visual Prompting for Robotic Manipulation using Foundation Models</span></a><br>
            <strong>Muhammad Angga Muttaqien</strong><br>
            <em>IEEE CASE 2025</em><br>
            <p>
              <strong>Abstract:</strong> A visual-language prompting method for robust robotic manipulation,
              combining GR00T and ACT models under variable environmental conditions.
            </p>
          </div>
        </div>
      </section>

      <!-- Projects -->
      <section class="section">
        <h2>Projects</h2>
        <ul>
          <li><strong>DreamerCL</strong> — Curriculum-guided world model learning for long-horizon embodied agents.</li>
          <li><strong>ALOHA + GR00T</strong> — Multimodal foundation model integration for robotic manipulation tasks.</li>
        </ul>
      </section>

      <footer>
        <p>© 2025 Muhammad Angga Muttaqien — Hosted on GitHub Pages</p>
      </footer>
    </div>
  </body>
</html>
